---
title: "Little Free Library Analysis"
author: "Kaleb Crans"
date: "2023-04-17"
output: pdf_document
---


```{r setup}
library(tidyverse)
library(readr)
library(sf)
library(spData)

rm(list = ls())
```

```{r}
typeof(read_csv("libraries.csv"))
if (file.exists("lfl.RData")) {
  load("lfl.RData")
} else {
  libraries = as_tibble(read_csv("libraries.csv"))
  save(libraries, file = "lfl.RData")
}
```

## Data Cleaning and Preparation

```{r}
lapply(libraries, typeof)
```

Map_Me__c needs to be transformed into a logical variable, as "Taken Down Temporarily" and "Mapped" are the only two categories.

```{r}
libraries <- libraries %>% mutate(Map_Me__c = Map_Me__c ==  "Mapped")
```


A quick look at check_in_counts shows that this feature is rarely used considering how many times a given library is actually visited:
```{r}
print("Max:")
max(libraries$check_in_count)
print("Summary Stats:")
summary(libraries$check_in_count)
```

```{r}
check_in_dist <- libraries  %>% count(check_in_count)
ggplot(data = libraries, aes(x =  check_in_count)) +
  ggtitle("Check-in count distribution") +
  xlab("Number of check-ins") +
  geom_histogram(binwidth = 1)
```


```{r}
# Clean up alternative names for the same country:
libraries <- libraries %>% mutate(Country__c = replace(Country__c, Country__c %in% c("USA", "US", "U.S.", "us", "Us"), "United States"))  %>% mutate(Country__c = replace(Country__c, Country__c == "canada", "Canada"))

# Ranking of countries by number of little free libraries:
libraries %>% count(Country__c) %>% arrange(desc(n))
```

The U.S. by and large has the greatest amount of little free libraries (with alternate spellings outpacing many countries even). Canada is the only country with a somewhat comparable amount, specifically if you adjust for population size.


```{r}
# US population
us_pop <- 331900000

# Canada population
can_pop <- 38250000

us_count <- libraries %>% filter(Country__c == "United States") %>% nrow

can_count <- libraries %>% filter(Country__c == "Canada") %>% nrow
```

So the per capita number of little free libraries in the US is:
```{r}
format(us_count/us_pop, scientific = FALSE)
```
And in Canada is:
```{r}
format(can_count/can_pop, scientific = FALSE)
```

So we can conclude that Little Free Libraries are a predominately American phenomenon. For the purposes of further analysis, let's exclude all data points not in the US:

```{r}
libraries <- libraries %>% filter(Country__c == "United States")
```

## Analysis by state

How about the distribution by state?

```{r}
length(unique(libraries$State_Province_Region__c))
```
But there's only 50 states! So we need to do some data cleaning first.

```{r}
libraries %>% count(State_Province_Region__c) %>% arrange(desc(n))
```

There's a bunch of different spelling variations. Let's instead take the actual coordinates and then find the states ourselves. One point of interest in the dataset to note that there are two sets of coordinates for each row: Latitude_MapAnything__c and Longitude_MapAnything__c vs Library_Geolocation__Latitude__s and Library_Geolocation__Longitude__s.

We can make a dataframe with the differences as separate columns, and print out the mean difference in latitude and longitude respectively:

```{r}
differences <- libraries %>% mutate(dif_lat = (abs(Latitude_MapAnything__c) - abs(Library_Geolocation__Latitude__s)), dif_long = abs(Longitude_MapAnything__c) - abs(Library_Geolocation__Longitude__s)) %>% select(dif_lat, dif_long)
c(mean(differences$dif_lat), mean(differences$dif_long))
```
Unfortunately due to the curvature of the earth these values don't mean too much as-is.

To explore further, let's take one example with a latitude difference of 16.6 and then plug the coordinates into Google maps. We get two different locations, one in Lake Park Iowa and the other in Lake Park Florida:
```{r}
libraries %>% filter(Latitude_MapAnything__c == 26.79489)
```
The MapAnything location:
![Lake Park in Florida](lake_florida.png)

The geolocation:

![Lake Park in Iowa](lake_iowa.png)
This library is actually displayed incorrectly in Iowa on the official webapp.

Another example is a location with a 101 degree difference in longitude. 

```{r}
libraries %>% filter(Latitude_MapAnything__c == 37.33889)
```
The actual location is in Kosovo, but because they put "KS" as the state (which is Kansas, not Kosovo) this row was mistakenly assigned "United States" as its country. 

The MapAnything location is in San Jose:
![Locaiton in San Jose](sanjose.png)

and the Geolocation is in Kosovo:
![Location in Kosovo](kosovo.png)

So we have two different examples where the correct coordinates are of different types. If we look at the distribution of coordinates we have: 

```{r}
libraries %>% select(Latitude_MapAnything__c, Library_Geolocation__Latitude__s, Longitude_MapAnything__c, Library_Geolocation__Longitude__s) %>% summary()
```
Thus summary statistics are similar, but there are enough differences to cause concern. Note that there are a decent amount of rows where the MapAnything coordinates are (0, 0):

```{r}
libraries %>% filter(Latitude_MapAnything__c == 0 & Longitude_MapAnything__c == 0) %>% count()
```
One example is the library with id 14180.
```{r}
libraries %>% filter(id == 14180)
```
None of the values look notable other than the (0, 0) MapAnything coordinates, and this library shows up on the official map.

These (0, 0) coordinates are basically missing values as all the libraries we are looking at are located in the US so (0, 0) is defintely an invalid coordinate. If we look at the webapp, it appears that the developers use the geolocation values on the interactive map:

![Screenshot of javascript snippet using Library_Geolocation for the pins on the map](latandlong.png)

So let's just use the geolocation coordinates exclusively then.

```{r}
libraries %>% select(Postal_Zip_Code__c) %>% unique %>% count
```


Let's drop every data point with coordinates not located within the United States.
We can look at what U.S. state a given library is in and then filter out the libraries
with no state values.

```{r}
# Convert the coordinates to a sf object
# Our coordinate reference system is the WGS84 standard which is what Google
# maps uses. Its EPSG Code is 4326. The format for a point is (longitude, latitude).
lib_pts <- libraries %>% st_as_sf(coords = c("Library_Geolocation__Longitude__s", "Library_Geolocation__Latitude__s"), crs = st_crs(4326))

lib_pts_alt <- libraries %>% st_as_sf(coords = c("Longitude_MapAnything__c", "Latitude_MapAnything__c"), crs = st_crs(4326))

GADM_data <- st_read(dsn = "gadm36_USA_gpkg/gadm36_USA.gpkg", layer = "gadm36_USA_1")
state_pts <- st_transform(GADM_data, crs = 4326)

state_names <- state_pts$NAME_1
classifications <- as.integer(st_intersects(lib_pts, state_pts))
alt_classifications <- as.integer(st_intersects(lib_pts_alt, state_pts))

libraries <- libraries %>% mutate(state = state_names[classifications])
classifs <- data.frame(state_names[classifications], state_names[alt_classifications])
sum(is.na(libraries$state))
classifications != alt_classifications
```
There are 123 location with coordinates not in the U.S. for whatever reason.
Let's take a look at them.

```{r}
libraries %>% filter(is.na(state))
```

Some of the rows are for foreign libraries, but it look like the majority are libraries with no street entries. Some like charter number G10014(148 Marina Plaza	Dunedin) are located very close to the ocean and hence were classified due to the resolution of the geography. A few like 150219 (1710 S Trenon Ave Tulsa) are mislabeled with coordinates not in the Unites States. Luckily there are only 123 entries, so we can filter them out without major concern.

```{r}
libraries <- libraries %>% drop_na(state)
```

Finally we can do some analysis with states:

```{r}
state_counts <- libraries %>% group_by(state) %>% count %>% arrange(desc(n))
state_counts <- state_counts %>% rename("NAME" = "state")
```

```{r}
st_transform(us_states, crs = 3857) %>% full_join(state_counts, by = join_by(NAME)) %>%
  ggplot(aes(fill = n)) +
  geom_sf()
```

Research Q's:

Is their a correlation between income in a given zip code and the number of little free libraries?
What about political part preference?
How about climate(i.e. colder areas might have less LFLs which are outdoors by design)




