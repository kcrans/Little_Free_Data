---
title: "Little Free Library Analysis"
author: "Kaleb Crans"
date: "2023-04-17"
output: pdf_document
---


```{r setup}
library(tidyverse)
library(readr)
library(sf)
library(spData)

rm(list = ls())
```

```{r}
typeof(read_csv("libraries.csv"))
if (file.exists("lfl.RData")) {
  load("lfl.RData")
} else {
  libraries = as_tibble(read_csv("libraries.csv"))
  save(libraries, file = "lfl.RData")
}
```

## Data Cleaning and Preparation

```{r}
lapply(libraries, typeof)
```

Map_Me__c needs to be transformed into a logical variable, as "Taken Down Temporarily" and "Mapped" are the only two categories.

```{r}
libraries <- libraries %>% mutate(Map_Me__c = Map_Me__c ==  "Mapped")
```


A quick look at check_in_counts shows that this feature is rarely used considering how many times a given library is actually visited:
```{r}
print("Max:")
max(libraries$check_in_count)
print("Summary Stats:")
summary(libraries$check_in_count)
```

```{r}
check_in_dist <- libraries  %>% count(check_in_count)
ggplot(data = libraries, aes(x =  check_in_count)) +
  ggtitle("Check-in count distribution") +
  xlab("Number of check-ins") +
  geom_histogram(binwidth = 1)
```


```{r}
# Clean up alternative names for the same country:
libraries <- libraries %>% mutate(Country__c = replace(Country__c, Country__c %in% c("USA", "US", "U.S.", "us", "Us"), "United States"))  %>% mutate(Country__c = replace(Country__c, Country__c == "canada", "Canada"))

# Ranking of countries by number of little free libraries:
libraries %>% count(Country__c) %>% arrange(desc(n))
```

The U.S. by and large has the greatest amount of little free libraries (with alternate spellings outpacing many countries even). Canada is the only country with a somewhat comparable amount, specifically if you adjust for population size.


```{r}
# US population
us_pop <- 331900000

# Canada population
can_pop <- 38250000

us_count <- libraries %>% filter(Country__c == "United States") %>% nrow

can_count <- libraries %>% filter(Country__c == "Canada") %>% nrow
```

So the per capita number of little free libraries in the US is:
```{r}
format(us_count/us_pop, scientific = FALSE)
```
And in Canada is:
```{r}
format(can_count/can_pop, scientific = FALSE)
```

So we can conclude that Little Free Libraries are a predominately American phenomenon. For the purposes of further analysis, let's exclude all data points not in the US:

```{r}
libraries <- libraries %>% filter(Country__c == "United States")
```

## Analysis by state

How about the distribution by state?

```{r}
length(unique(libraries$State_Province_Region__c))
```
But there's only 50 states! So we need to do some data cleaning first.

```{r}
libraries %>% count(State_Province_Region__c) %>% arrange(desc(n))
```

There's a bunch of different spelling variations. Let's instead take the actual coordinates and then find the states ourselves. One point of interest in the dataset to note that there are two sets of coordinates for each row: Latitude_MapAnything__c and Longitude_MapAnything__c vs Library_Geolocation__Latitude__s and Library_Geolocation__Longitude__s.

We can make a dataframe with the differences as separate columns, and print out the mean difference in latitude and longitude respectively:

```{r}
differences <- libraries %>% mutate(dif_lat = (abs(Latitude_MapAnything__c) - abs(Library_Geolocation__Latitude__s)), dif_long = abs(Longitude_MapAnything__c) - abs(Library_Geolocation__Longitude__s)) %>% select(dif_lat, dif_long)
c(mean(differences$dif_lat), mean(differences$dif_long))
```
Unfortunately due to the curvature of the earth these values don't mean too much as-is.

To explore further, let's take one example with a latitude difference of 16.6 and then plug the coordinates into Google maps. We get two different locations, one in Lake Park Iowa and the other in Lake Park Florida:
```{r}
libraries %>% filter(Latitude_MapAnything__c == 26.79489)
```

The MapAnything location:

![Lake Park in Florida](lake_florida.png)

The geolocation:

![Lake Park in Iowa](lake_iowa.png)

This library is actually displayed incorrectly in Iowa on the official webapp.

Another example is a location with a 101 degree difference in longitude. 

```{r}
libraries %>% filter(Latitude_MapAnything__c == 37.33889)
```
The actual location is in Kosovo, but because they put "KS" as the state (which is Kansas, not Kosovo) this row was mistakenly assigned "United States" as its country. 

The MapAnything location is in San Jose:

![Location in San Jose](sanjose.png)

and the Geolocation is in Kosovo:

![Location in Kosovo](kosovo.png)


So we have two different examples where the correct coordinates are of different types. If we look at the distribution of coordinates we have: 

```{r}
libraries %>% select(Latitude_MapAnything__c, Library_Geolocation__Latitude__s, Longitude_MapAnything__c, Library_Geolocation__Longitude__s) %>% summary()
```
Thus summary statistics are similar, but there are enough differences to cause concern. Note that there are a decent amount of rows where the MapAnything coordinates are (0, 0):

```{r}
libraries %>% filter(Latitude_MapAnything__c == 0 & Longitude_MapAnything__c == 0) %>% count()
```
One example is the library with id 14180.
```{r}
libraries %>% filter(id == 14180)
```
None of the values look notable other than the (0, 0) MapAnything coordinates, and this library shows up on the official map.

These (0, 0) coordinates are basically missing values as all the libraries we are looking at are located in the US so (0, 0) is defintely an invalid coordinate. If we look at the webapp, it appears that the developers use the geolocation values on the interactive map:

![Screenshot of javascript snippet using Library_Geolocation for the pins on the map](latandlong.png)

So let's just use the geolocation coordinates exclusively then.

```{r}
libraries %>% select(Postal_Zip_Code__c) %>% unique %>% count
```


Let's drop every data point with coordinates not located within the United States.
We can look at what U.S. state a given library is in and then filter out the libraries
with no state values.

```{r}
# Convert the coordinates to a sf object
# Our coordinate reference system is the WGS84 standard which is what Google
# maps uses. Its EPSG Code is 4326. The format for a point is (longitude, latitude).
lib_pts <- libraries %>% st_as_sf(coords = c("Library_Geolocation__Longitude__s", "Library_Geolocation__Latitude__s"), crs = st_crs(4326))

# For comparison also convert the MapAnything coordinates into a sf object
lib_pts_alt <- libraries %>% st_as_sf(coords = c("Longitude_MapAnything__c", "Latitude_MapAnything__c"), crs = st_crs(4326))

# Read in and transform the GADM data to WGS84 format. 
GADM_data <- st_read(dsn = "gadm36_USA_gpkg/gadm36_USA.gpkg", layer = "gadm36_USA_1")
state_pts <- st_transform(GADM_data, crs = 4326)

# Make a data.frame of all the possible state names
state_names <- state_pts$NAME_1

# Find the intersections between the library points and state polygons
# Convert to an integer to use as an index in the state names data.frame.
classifications <- as.integer(st_intersects(lib_pts, state_pts))
alt_classifications <- as.integer(st_intersects(lib_pts_alt, state_pts))

temp_libraries <- libraries %>% mutate(
  state = state_names[classifications], alt_state = state_names[alt_classifications])
sum(is.na(temp_libraries$state))
```
There are 123 location with (geolocation) coordinates not in the U.S. for whatever reason.
Let's take a look at them.

```{r}
temp_libraries %>% filter(is.na(state))
```

Some of the rows are for foreign libraries, but it look like the majority are libraries with no street entries. Some like charter number G10014(148 Marina Plaza	Dunedin) are located very close to the ocean and hence were classified incorrectly due to the resolution of the geography. A few like 150219 (1710 S Trenon Ave Tulsa) are mislabeled with coordinates not in the Unites States. By manual inspection, it looks the MapAnything coordinates give the appropriate state for some of the cases where the locations are right near a body of water or the street address is missing. When a library is located in a country other than the United States, both columns will have NA values. In general, if we take a look at the state assignments based off the geolocation coordinates ("state") and MapAnything coordinates ("alt_state"), the possibilities are:

1. NA for both. This means either the library is not in the U.S and should be removed from our dataframe, or it is located in the U.S. but too close to the ocean. We filter out data in the former instance and use the "State_Province_Region__c" assignment in the latter.

2. An actual state for the geolocation coordinates and NA for MapAnything. We should use the "state" assignment then.

3. An actual state for the MapAnything coordinates and NA for the geolocation. We will go with the "alt_state" assignment in this case.

4. and 5. Actual states for both coordinates. If they are the same, we will go with that assignment. If they are different, we will use whichever assignment lines up with the "State_Province_Region__c" value for that datapoint.

Now, let's create a new column state_name to hold whatever state name based off the above criteria, and also long and lat columns to hold the coordinates we end up using.

First, here's a helper function to convert state abbreviations into full names:
```{r}
convert_state <- function(state_code) {
  name_str <- switch(state_code,
  "AL" = "Alabama",
  "AK" = "Alaska",
  "AZ" = "Alaska",
  "AR" = "Arkansas",
  "AS" = "American Samoa",
  "CA" = "California",
  "CO" = "Colorado",
  "CT" = "Connecticut",
  "DE" = "Delaware",
  "DC" = "District of Columbia",
  "FL" = "Florida",
  "GA" = "Georgia",
  "GU" = "Guam",
  "HI" = "Hawaii",
  "ID" = "Idaho",
  "IL" = "Illinois",
  "IN" = "Indiana",
  "IA" = "Iowa",
  "KS" = "Kansas",
  "KY" = "Kentucky",
  "LA" = "Louisiana",
  "ME" = "Maine",
  "MD" = "Maryland",
  "MA" = "Massachusetts",
  "MI" = "Michigan",
  "MN" = "Minnesota",
  "MS" = "Mississippi",
  "MO" = "Missouri",
  "MT" = "Montana",
  "NE" = "Nebraska",
  "NV" = "Nevada",
  "NH" = "New Hampshire",
  "NJ" = "New Jersey",
  "NM" = "New Mexico",
  "NY" = "New York",
  "NC" = "North Carolina",
  "ND" = "North Dakota",
  "MP" = "Northern Mariana Islands",
  "OH" = "Ohio",
  "OK" = "Oklahoma",
  "OR" = "Oregon",
  "PA" = "Pennsylvania",
  "PR" = "Puerto Rico",
  "RI" = "Rhode Island",
  "SC" = "South Carolina",
  "SD" = "South Dakota",
  "TD" = "Tennessee",
  "TX" = "Texas",
  "TT" = "Trust Territories",
  "UT" = "Utah",
  "VT" = "Vermont",
  "VA" = "Virginia",
  "VI" = "Virgin Islands",
  "WA" = "Washington",
  "WV" = "West Virginia",
  "WI" = "Wisconsin",
  "WY" = "Wyoming"
  )
  return(name_str)
}
convert_state <- Vectorize(convert_state)
```


1.
```{r}
temp_libraries %>% filter(is.na(state) & is.na(alt_state)) %>% filter(!is.na(State_Province_Region__c))
```
The library in Alytus, Lithuania is the only one not in the U.S. So let's just drop that one and add the new state assignments:

```{r}
case_1 <- temp_libraries %>% filter(is.na(state) & is.na(alt_state)) %>% filter(!is.na(State_Province_Region__c)) %>%
  filter(City__c != "Alytus") %>% mutate(state_name = convert_state(State_Province_Region__c), long = Library_Geolocation__Longitude__s, lat = Library_Geolocation__Latitude__s)
```

2.
```{r}
case_2 <- temp_libraries %>% filter(!is.na(state) & is.na(alt_state)) %>%
  mutate(state_name = state, long = Library_Geolocation__Longitude__s, lat = Library_Geolocation__Latitude__s)
```

3.
```{r}
case_3 <- temp_libraries %>% filter(is.na(state) & !is.na(alt_state)) %>%
  mutate(state_name = alt_state, long = Longitude_MapAnything__c, lat = Latitude_MapAnything__c)
```


4.
```{r}
case_4 <- temp_libraries %>% filter(state == alt_state) %>% mutate(state_name = state, long = Library_Geolocation__Longitude__s, lat = Library_Geolocation__Latitude__s)
```

5.
```{r}
case_5a <- temp_libraries %>% filter(state != alt_state ) %>% mutate(State_Province_Region__c = convert_state(State_Province_Region__c)) %>% filter(State_Province_Region__c == state) %>% mutate(state_name = state, long = Library_Geolocation__Longitude__s, lat = Library_Geolocation__Latitude__s)
```

```{r}
case_5b <- temp_libraries %>% filter(state != alt_state ) %>% mutate(State_Province_Region__c = convert_state(State_Province_Region__c)) %>% filter(State_Province_Region__c == alt_state) %>% mutate(state_name = alt_state, long = Longitude_MapAnything__c, lat = Latitude_MapAnything__c)
```

Joining it all together:

```{r}
temp_libraries <- bind_rows(case_1, case_2, case_3, case_4, case_5a, case_5b)
libraries <- select(temp_libraries, -c(state, alt_state, State_Province_Region__c, Library_Geolocation__Longitude__s, Library_Geolocation__Latitude__s, Longitude_MapAnything__c, Latitude_MapAnything__c )) %>% arrange(id)
```


** Then drop state, alt_state, and both original coordinates

** Then sort rows based off id

** Then store as library


Finally we can do some analysis with states:

```{r}
state_counts <- libraries %>% group_by(state_name) %>% count %>% arrange(desc(n))
state_counts <- state_counts %>% rename("NAME" = "state_name")
```

```{r}
st_transform(us_states, crs = 3857) %>% full_join(state_counts, by = join_by(NAME)) %>%
  ggplot(aes(fill = n)) +
  geom_sf()
```

Research Q's:

Is their a correlation between income in a given zip code and the number of little free libraries?
What about political party preference?
How about climate(i.e. colder areas might have less LFLs which are outdoors by design)




