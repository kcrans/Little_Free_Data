---
title: "Little Free Library Analysis"
author: "Kaleb Crans"
date: "2023-04-17"
output: pdf_document
---


```{r setup}
library(tidyverse)
library(readr)
library(sf)
library(spData)

rm(list = ls())
```

```{r}
typeof(read_csv("libraries.csv"))
if (file.exists("lfl.RData")) {
  load("lfl.RData")
} else {
  libraries = as_tibble(read_csv("libraries.csv"))
  save(libraries, file = "lfl.RData")
}
```

## Data Cleaning and Preparation

Let's take a look at the different data types of the columns:
```{r}
lapply(libraries, typeof)
```

Map_Me__c needs to be transformed into a logical variable, as "Taken Down Temporarily" and "Mapped" are the only two categories.

```{r}
libraries <- libraries %>% mutate(Map_Me__c = Map_Me__c ==  "Mapped")
```


A quick look at check_in_counts shows that this feature is rarely used when you consider how many times a given library is actually visited:
```{r}
print("Max:")
max(libraries$check_in_count)
print("Summary Stats:")
summary(libraries$check_in_count)
```

```{r}
check_in_dist <- libraries  %>% count(check_in_count)
ggplot(data = libraries, aes(x =  check_in_count)) +
  ggtitle("Check-in count distribution") +
  xlab("Number of check-ins") +
  geom_histogram(binwidth = 1)
```


```{r}
# Clean up alternative names for the same country:
libraries <- libraries %>% mutate(Country__c = replace(Country__c, Country__c %in% c("USA", "US", "U.S.", "us", "Us"), "United States"))  %>% mutate(Country__c = replace(Country__c, Country__c == "canada", "Canada"))

# Ranking of countries by number of little free libraries:
libraries %>% count(Country__c) %>% arrange(desc(n))
```

The U.S. by and large has the greatest amount of little free libraries (with alternate spellings outpacing many countries even). Canada is the only country with a somewhat comparable amount, specifically if you adjust for population size.


```{r}
# US population
us_pop <- 331900000

# Canada population
can_pop <- 38250000

us_count <- libraries %>% filter(Country__c == "United States") %>% nrow

can_count <- libraries %>% filter(Country__c == "Canada") %>% nrow
```

So the per capita number of little free libraries in the US is:
```{r}
format(us_count/us_pop, scientific = FALSE)
```
And in Canada is:
```{r}
format(can_count/can_pop, scientific = FALSE)
```


```{r}
libraries %>% filter(is.na(Country__c) & !is.na(State_Province_Region__c))
```
So we can conclude that Little Free Libraries are a predominately American phenomenon. For the purposes of further analysis, let's exclude all data points not in the US. We will also include rows that have a NA country value (but still a state code) as on inspection many of them are actually located in the US. We can impute the correct country code based off the coordinates later.

```{r}
libraries <- libraries %>% filter(Country__c == "United States" | (is.na(Country__c) & !is.na(State_Province_Region__c)))
```

## Analysis by state

How about the distribution by state?

```{r}
length(unique(libraries$State_Province_Region__c))
```
But there's only 50 states! So we need to do some data cleaning first.

```{r}
libraries %>% count(State_Province_Region__c) %>% arrange(desc(n))
```

There's a bunch of different spelling variations. Let's instead take the actual coordinates and then find the states ourselves. One point of interest in the dataset to note is that there are two sets of coordinates for each row: Latitude_MapAnything__c and Longitude_MapAnything__c vs Library_Geolocation__Latitude__s and Library_Geolocation__Longitude__s.

We can make a dataframe with the differences as separate columns, and print out the mean difference in latitude and longitude respectively:

```{r}
differences <- libraries %>% mutate(dif_lat = (abs(Latitude_MapAnything__c) - abs(Library_Geolocation__Latitude__s)), dif_long = abs(Longitude_MapAnything__c) - abs(Library_Geolocation__Longitude__s)) %>% select(dif_lat, dif_long)
c(mean(differences$dif_lat), mean(differences$dif_long))
```
Unfortunately due to the curvature of the earth these values don't mean too much as-is.

To explore further, let's take one example with a latitude difference of 16.6 and then plug the coordinates into Google maps. We get two different locations, one in Lake Park Iowa and the other in Lake Park Florida:
```{r}
libraries %>% filter(Latitude_MapAnything__c == 26.79489)
```

The MapAnything location:

![Lake Park in Florida](lake_florida.png)

The geolocation:

![Lake Park in Iowa](lake_iowa.png)

This library is actually displayed incorrectly in Iowa on the official webapp.

Another example is a location with a 101 degree difference in longitude. 

```{r}
libraries %>% filter(Latitude_MapAnything__c == 37.33889)
```
The actual location is in Kosovo, but because they put "KS" as the state (which is Kansas, not Kosovo) this row was mistakenly assigned "United States" as its country. 

The MapAnything location is in San Jose:

![Location in San Jose](sanjose.png)

and the Geolocation is in Kosovo:

![Location in Kosovo](kosovo.png)


So we have two different examples where the correct coordinates are of different types. If we look at the distribution of coordinates we have: 

```{r}
libraries %>% select(Latitude_MapAnything__c, Library_Geolocation__Latitude__s, Longitude_MapAnything__c, Library_Geolocation__Longitude__s) %>% summary()
```
Thus summary statistics are similar, but there are enough differences to cause concern. Note that there are a decent amount of rows where the MapAnything coordinates are (0, 0):

```{r}
libraries %>% filter(Latitude_MapAnything__c == 0 & Longitude_MapAnything__c == 0) %>% count()
```
One example is the library with id 14180.
```{r}
libraries %>% filter(id == 14180)
```
None of the values look notable other than the (0, 0) MapAnything coordinates, and this library shows up on the official map.

These (0, 0) coordinates are basically missing values as all the libraries we are looking at are located in the US so (0, 0) is definitely an invalid coordinate. If we look at the webapp, it appears that the developers use the geolocation values on the interactive map:

![Screenshot of javascript snippet using Library_Geolocation for the pins on the map](latandlong.png)

So let's default to the geolocation coordinates, but use the MapAnythin coordinates if they match up with the address listed.


Let's drop every data point with coordinates not located within the United States.
We can look at what U.S. state a given library is in and then filter out the libraries
with no state values.

```{r}
# Convert the coordinates to a sf object
# Our coordinate reference system is the WGS84 standard which is what Google
# maps uses. Its EPSG Code is 4326. The format for a point is (longitude, latitude).
lib_pts <- libraries %>% st_as_sf(coords = c("Library_Geolocation__Longitude__s", "Library_Geolocation__Latitude__s"), crs = st_crs(4326))

# For comparison also convert the MapAnything coordinates into a sf object
lib_pts_alt <- libraries %>% st_as_sf(coords = c("Longitude_MapAnything__c", "Latitude_MapAnything__c"), crs = st_crs(4326))

# Read in and transform the GADM data to WGS84 format. 
GADM_data <- st_read(dsn = "gadm36_USA_gpkg/gadm36_USA.gpkg", layer = "gadm36_USA_1")
state_pts <- st_transform(GADM_data, crs = 4326)

# Make a data.frame of all the possible state names
state_names <- state_pts$NAME_1

# Find the intersections between the library points and state polygons
# Convert to an integer to use as an index in the state names data.frame.
classifications <- as.integer(st_intersects(lib_pts, state_pts))
alt_classifications <- as.integer(st_intersects(lib_pts_alt, state_pts))

temp_libraries <- libraries %>% mutate(
  state = state_names[classifications], alt_state = state_names[alt_classifications])
sum(is.na(temp_libraries$state))
```
There are 166 locations with (geolocation) coordinates not in the U.S. for whatever reason.
Let's take a look at them.

```{r}
temp_libraries %>% filter(is.na(state) & is.na(alt_state))
```

Some of the rows are for foreign libraries, but it look like the majority are libraries with no street entries. Some like charter number G10014(148 Marina Plaza	Dunedin) are located very close to the ocean and hence were classified incorrectly due to the resolution of the geography. A few like 150219 (1710 S Trenon Ave Tulsa) are mislabeled with coordinates not in the Unites States. By manual inspection, it looks the MapAnything coordinates give the appropriate state for some of the cases where the locations are right near a body of water or the street address is missing. When a library is located in a country other than the United States, both columns will have NA values. In general, if we take a look at the state assignments based off the geolocation coordinates ("state") and MapAnything coordinates ("alt_state"), the possibilities are:

1. NA for both. This means either the library is not in the U.S and should be removed from our dataframe, or it is located in the U.S. but too close to the ocean. We filter out data in the former instance and use the "State_Province_Region__c" assignment in the latter.

2. An actual state for the geolocation coordinates and NA for MapAnything. We should use the "state" assignment then.

3. An actual state for the MapAnything coordinates and NA for the geolocation. We will go with the "alt_state" assignment in this case.

4. and 5. Actual states for both coordinates. If they are the same, we will go with that assignment. If they are different, we will use whichever assignment lines up with the "State_Province_Region__c" value for that datapoint.

Now, let's create a new column state_name to hold whatever state name based off the above criteria, and also long and lat columns to hold the coordinates we end up using.

First, here's a helper function to convert state abbreviations into full names:
```{r}
convert_state <- function(state_code) {
  up_code <- toupper(state_code) # Ignore case differences
  name_str <- switch(up_code,
  "AL" = "Alabama",
  "AK" = "Alaska",
  "AZ" = "Alaska",
  "AR" = "Arkansas",
  "AS" = "American Samoa",
  "CA" = "California",
  "CO" = "Colorado",
  "CT" = "Connecticut",
  "DE" = "Delaware",
  "DC" = "District of Columbia",
  "FL" = "Florida",
  "GA" = "Georgia",
  "GU" = "Guam",
  "HI" = "Hawaii",
  "ID" = "Idaho",
  "IL" = "Illinois",
  "IN" = "Indiana",
  "IA" = "Iowa",
  "KS" = "Kansas",
  "KY" = "Kentucky",
  "LA" = "Louisiana",
  "ME" = "Maine",
  "MD" = "Maryland",
  "MA" = "Massachusetts",
  "MI" = "Michigan",
  "MN" = "Minnesota",
  "MS" = "Mississippi",
  "MO" = "Missouri",
  "MT" = "Montana",
  "NE" = "Nebraska",
  "NV" = "Nevada",
  "NH" = "New Hampshire",
  "NJ" = "New Jersey",
  "NM" = "New Mexico",
  "NY" = "New York",
  "NC" = "North Carolina",
  "ND" = "North Dakota",
  "MP" = "Northern Mariana Islands",
  "OH" = "Ohio",
  "OK" = "Oklahoma",
  "OR" = "Oregon",
  "PA" = "Pennsylvania",
  "PR" = "Puerto Rico",
  "RI" = "Rhode Island",
  "SC" = "South Carolina",
  "SD" = "South Dakota",
  "TN" = "Tennessee",
  "TX" = "Texas",
  "TT" = "Trust Territories",
  "UT" = "Utah",
  "VT" = "Vermont",
  "VA" = "Virginia",
  "VI" = "Virgin Islands",
  "WA" = "Washington",
  "WV" = "West Virginia",
  "WI" = "Wisconsin",
  "WY" = "Wyoming",
  state_code
  )
  return(name_str)
}
convert_state <- Vectorize(convert_state)
```


1.
```{r}
 temp_libraries %>% filter(is.na(state) & is.na(alt_state)) %>% filter(!is.na(State_Province_Region__c) & !is.na(Country__c))
```
The library in Alytus, Lithuania is the only one not in the U.S. So let's just drop that one and add the new state assignments:

```{r}
case_1 <- temp_libraries %>% filter(is.na(state) & is.na(alt_state)) %>% filter(!is.na(State_Province_Region__c) & !is.na(Country__c)) %>%
  filter(City__c != "Alytus") %>% mutate(state_name = convert_state(State_Province_Region__c), long = Library_Geolocation__Longitude__s, lat = Library_Geolocation__Latitude__s)
```

2.
```{r}
case_2 <- temp_libraries %>% filter(!is.na(state) & is.na(alt_state)) %>%
  mutate(state_name = state, long = Library_Geolocation__Longitude__s, lat = Library_Geolocation__Latitude__s)
```

3.
```{r}
case_3 <- temp_libraries %>% filter(is.na(state) & !is.na(alt_state)) %>%
  mutate(state_name = alt_state, long = Longitude_MapAnything__c, lat = Latitude_MapAnything__c)
```


4.
```{r}
case_4 <- temp_libraries %>% filter(state == alt_state) %>% mutate(state_name = state, long = Library_Geolocation__Longitude__s, lat = Library_Geolocation__Latitude__s)
```


5.
```{r}
case_5a <- temp_libraries %>% filter(state != alt_state ) %>% mutate(State_Province_Region__c = convert_state(State_Province_Region__c)) %>% filter(State_Province_Region__c == state) %>% mutate(state_name = state, long = Library_Geolocation__Longitude__s, lat = Library_Geolocation__Latitude__s)
```

```{r}
case_5b <- temp_libraries %>% filter(state != alt_state ) %>%  mutate(State_Province_Region__c = convert_state(State_Province_Region__c)) %>% filter(State_Province_Region__c == alt_state) %>% mutate(state_name = alt_state, long = Longitude_MapAnything__c, lat = Latitude_MapAnything__c)
```

Joining it all together:

```{r}
temp_libraries <- bind_rows(case_1, case_2, case_3, case_4, case_5a, case_5b)
# Drop state, alt_state, and both original coordinates
libraries <- select(temp_libraries, -c(state, alt_state, State_Province_Region__c, Library_Geolocation__Longitude__s, Library_Geolocation__Latitude__s, Longitude_MapAnything__c, Latitude_MapAnything__c )) %>% 
  arrange(id) # Sort rows based off id
```

Finally we can do some analysis with states:

```{r}
state_counts <- libraries %>% group_by(state_name) %>% count %>% arrange(desc(n))
state_counts <- state_counts %>% rename("NAME" = "state_name")
```

```{r}
state_counts %>% ggplot(aes(x = n)) +
  geom_histogram(binwidth = 1000)
```


```{r}
st_transform(us_states, crs = 3857) %>% full_join(state_counts, by = join_by(NAME)) %>%
  ggplot(aes(fill = n)) +
  geom_sf() + 
  labs(fill = "Number of LFLs") + 
  scale_fill_fermenter(n.breaks = 9, direction = 1, palette = "Oranges") +
  ggtitle("Number of Little Free Libraries per state")
```
And then if we normalize by population:

```{r}
state_pops = as_tibble(read_csv("census_pop_data.csv"))
state_pops <- state_pops[1,] %>% select(-"Label (Grouping)" ) %>% pivot_longer(cols = everything(), names_to = "NAME", values_to = "pop")
norm_state_counts <- state_counts %>% left_join(state_pops) %>% mutate(per_cap = 10000*n/pop) %>% select(!c(n, pop))
```

```{r}
st_transform(us_states, crs = 3857) %>% left_join(norm_state_counts, by = join_by(NAME)) %>%
  ggplot(aes(fill = per_cap)) +
  geom_sf() + 
  labs(fill = "LFLs per 10,000 residents") + 
  scale_fill_fermenter(n.breaks = 9, direction = 1, palette = "Oranges") +
  ggtitle("Number of Little Free Libraries (population normalized)")
```
Per capita, it looks like the distribution of LFLs is biased towards the Midwest, in particular Minnesota and Wisconsin. This is not surprising, as the organization was founded in Hudson, Wisconsin and eventually moved to Minneapolis, Minnesota.


## Analysis by zipcode

There are many faults to doing geospatial analysis based off zipcodes (they overlap, are not contiguous, etc...) but let's just take a quick look:

```{r}
library(tidycensus)
census_api_key("7ce885ce139c2116d8d26ffca665df473f98a98c")
```
```{r}
county_incomes <- get_acs(geography = "county", variables = "B19013_001", year = 2021)
```
```{r}
# Convert the coordinates to a sf object
# Our coordinate reference system is the WGS84 standard which is what Google
# maps uses. Its EPSG Code is 4326. The format for a point is (longitude, latitude).
lib_pts <- libraries %>% st_as_sf(coords = c("long", "lat"), crs = st_crs(4326))

# Read in and transform the GADM data to WGS84 format. 
GADM_data2 <- st_read(dsn = "gadm36_USA_gpkg/gadm36_USA.gpkg", layer = "gadm36_USA_2")
county_pts <- st_transform(GADM_data2, crs = 4326)

# Make a data.frame of all the possible state names
county_names <- county_pts$NAME_2

# Find the intersections between the library points and state polygons
# Convert to an integer to use as an index in the state names data.frame.
classifications <- as.integer(st_intersects(lib_pts, county_pts))

temp_libraries <- libraries %>% mutate(
  county = county_names[classifications])
#sum(is.na(temp_libraries$state))
```

```{r}
# Read in and transform the GADM data to WGS84 format. 
GADM_data2 <- st_read(dsn = "gadm36_USA_gpkg/gadm36_USA.gpkg", layer = "gadm36_USA_2")
county_pts <- st_transform(GADM_data2, crs = 4326)
```
```{r}
county_names
```

Research Q's:

Is their a correlation between income in a given zip code and the number of little free libraries?
What about political party preference?
How about climate(i.e. colder areas might have less LFLs which are outdoors by design)




